{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":89993,"databundleVersionId":10699058,"sourceType":"competition"},{"sourceId":336169,"sourceType":"modelInstanceVersion","modelInstanceId":281373,"modelId":302265}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"!pip install ultralytics torch torchvision torchaudio facenet-pytorch\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport cv2\nimport os\nimport ast\nfrom tqdm import tqdm\nfrom ultralytics import YOLO\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom sklearn.model_selection import train_test_split\nfrom facenet_pytorch import InceptionResnetV1","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Configurations","metadata":{}},{"cell_type":"code","source":"DATASET_PATH = \"/kaggle/input/surveillance-for-retail-stores/face_identification/face_identification/\"\nTRAINSET_CSV = f\"{DATASET_PATH}trainset.csv\"\nTEST_PATH = f\"{DATASET_PATH}test/\"\nEVAL_SET_CSV = f\"{DATASET_PATH}eval_set.csv\"\nYOLO_MODEL_PATH = \"/kaggle/input/tracking/tensorflow2/default/1/best.pt\"\n\n# Model parameters\nEMBEDDING_SIZE = 512\nFACE_SIZE = 160  # Facenet expects 160x160\n\n# Training parameters\n# EXTRACTOR_EPOCHS = 50\n# BATCH_SIZE = 16\n# LEARNING_RATE = 0.001\n\n# Training parameters\nEXTRACTOR_EPOCHS = 10\nBATCH_SIZE = 8\nLEARNING_RATE = 0.01","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T21:59:54.529520Z","iopub.execute_input":"2025-04-18T21:59:54.529934Z","iopub.status.idle":"2025-04-18T21:59:54.534599Z","shell.execute_reply.started":"2025-04-18T21:59:54.529910Z","shell.execute_reply":"2025-04-18T21:59:54.533906Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load pre-trained YOLO model","metadata":{}},{"cell_type":"code","source":"print(\"Loading pre-trained YOLO model...\")\ndetector = YOLO(YOLO_MODEL_PATH)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Prepare face dataset\n- Reads images\n- Uses the YOLO model to detect & crop the face inside each image\n- Applies image transformations (resizing to 160x160 pixels, normalizing colors) to prepare for training.","metadata":{}},{"cell_type":"code","source":"class FaceDataset(Dataset):\n    def __init__(self, dataframe):\n        self.dataframe = dataframe\n        self.transform = transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.Resize((FACE_SIZE, FACE_SIZE)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n        ])\n        \n    def __len__(self):\n        return len(self.dataframe)\n    \n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n        img_path = f\"{DATASET_PATH}{row['image_path']}\"\n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        # Use YOLO to detect and crop face\n        results = detector(image)\n        if len(results[0].boxes) > 0:\n            x1, y1, x2, y2 = map(int, results[0].boxes.xyxy[0].tolist())\n            image = image[y1:y2, x1:x2]\n        \n        if self.transform:\n            image = self.transform(image)\n            \n        return image, row['gt']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T21:59:58.430710Z","iopub.execute_input":"2025-04-18T21:59:58.431033Z","iopub.status.idle":"2025-04-18T21:59:58.437234Z","shell.execute_reply.started":"2025-04-18T21:59:58.431016Z","shell.execute_reply":"2025-04-18T21:59:58.436679Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Create a Feature Extractor (Fine-Tuned Face Recog Model)\n- Starts from a pre-trained FaceNet model (trained on VGGFace2 dataset)\n- Adds a new classification layer (name tags)\n- Fine-tunes it to specifically recognize faces in training set\n\n### Training process\n- Adjusting the model with:\n    - **Loss function:** to measure mistakes (CrossEntropyLoss). Cross-entropy loss is standard for classification tasks, as it measures the difference between predicted probabilities and true labels.\n    - **Optimizer:** Learning algorithm Adam with a certain learning rate.\n- Validates with 20% unseen images to check progress","metadata":{}},{"cell_type":"code","source":"class FineTunedFaceNet(nn.Module):\n    def __init__(self, num_classes=None):\n        super().__init__()\n        self.base = InceptionResnetV1(pretrained='vggface2') # base facenet model trained on VGGFace2\n        if num_classes:\n            self.classifier = nn.Linear(512, num_classes)\n        else:\n            self.classifier = None\n    \n    def forward(self, x):\n        features = self.base(x)\n        if self.classifier:\n            return self.classifier(features)\n        return features\n\ndef train_feature_extractor():\n    print(\"=== Training Feature Extractor ===\")\n    \n    # Prepare dataset\n    trainset = pd.read_csv(TRAINSET_CSV)\n    label_to_idx = {label: idx for idx, label in enumerate(trainset['gt'].unique())} # create a dict with person_number : running index (0, 1, ...)\n    trainset['label_idx'] = trainset['gt'].map(label_to_idx) # replace each value in gt column with its numeric index using label_to_idx dictionary\n\n    # split into training & validatio\n    train_df, val_df = train_test_split(trainset, test_size=0.2, stratify=trainset['label_idx'])\n    \n    train_dataset = FaceDataset(train_df)\n    val_dataset = FaceDataset(val_df)\n    # dataloaders allow efficient iteration and batching\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n    \n    # Initialize Model: Sets up the neural network, loss function, optimizer, and hardware device. \n    model = FineTunedFaceNet(len(label_to_idx)) # classifier head for N persons\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # compute with gpu if found\n    model = model.to(device)\n    criterion = nn.CrossEntropyLoss() # loss function\n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE) # optimizer\n    \n    # Training Loop: Iteratively updates the model's weights using training data.\n    for epoch in range(EXTRACTOR_EPOCHS):\n        model.train() # Set the model to training mode, enabling features like dropout and batch normalization\n        \n        # Iterates over batches of images and labels from the training DataLoader\n        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"): \n            # Converts string labels (labels) to numeric indices using label_to_idx and moves them to the GPU/CPU.\n            labels = torch.tensor([label_to_idx[l] for l in labels]).to(device) \n            images = images.to(device)\n\n            # Clear old gradients before the next batch to prevent accumulation.\n            optimizer.zero_grad()\n            # forward pass\n            outputs = model(images)\n            # compute loss by comparing output with ground truth (labels)\n            loss = criterion(outputs, labels)\n            # Computes gradients of the loss with respect to the model's parameters (backpropagate)\n            loss.backward()\n            # Update the model's weights using the computed gradients\n            optimizer.step()\n        \n        # Validation: Evaluates model performance on unseen data to monitor generalization.\n        # set model to eval mode\n        model.eval()\n        correct = 0\n        total = 0\n        # disable gradient to save memory & speed\n        with torch.no_grad():\n            # iterate over validation batches, same as training loop but without backpropagation\n            for images, labels in val_loader:\n                labels = torch.tensor([label_to_idx[l] for l in labels]).to(device)\n                images = images.to(device)\n                outputs = model(images)\n                # Extracts the predicted class (the one with the highest probability)\n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n        \n        print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")\n    \n    return model, label_to_idx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T21:59:58.437978Z","iopub.execute_input":"2025-04-18T21:59:58.438222Z","iopub.status.idle":"2025-04-18T21:59:58.459139Z","shell.execute_reply.started":"2025-04-18T21:59:58.438199Z","shell.execute_reply":"2025-04-18T21:59:58.458268Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"extractor, label_to_idx = train_feature_extractor()\nidx_to_label = {v: k for k, v in label_to_idx.items()} # convert from ids to labels","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Create embeddings Database\n1. Detect their face\n2. Crop & resize it\n3. Run it through the trained model to get a 512-dimensional vector (an embedding) that uniquely represents the face\n4. Save these embeddings in a dict","metadata":{}},{"cell_type":"code","source":"trainset = pd.read_csv(TRAINSET_CSV)\nembeddings_dict = {}\n\nextractor.eval()\ntransform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((FACE_SIZE, FACE_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\nwith torch.no_grad():\n    for idx, row in tqdm(trainset.iterrows(), total=len(trainset), desc=\"Creating embeddings\"):\n        img_path = f\"{DATASET_PATH}{row['image_path']}\"\n        person = row[\"gt\"]\n        \n        # Detect face\n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        results = detector(image)\n        \n        if len(results[0].boxes) == 0:\n            continue\n            \n        # Crop face\n        x1, y1, x2, y2 = map(int, results[0].boxes.xyxy[0].tolist())\n        face_img = image[y1:y2, x1:x2]\n        \n        # Get embedding\n        # Applies the image transformations defined earlier (transforms.Compose) to the cropped face image (face_img).\n        # the transform(face_img) outputs a 3D array [Channels, Height, Width] & torch expects input as 4D array [Batch_Size, Channels, Height, Width]\n        # so unsqueeze(0) makes Batch_Size = 1 so that the model doesn't crash while processing, otherwise an error might be returned\n        face_tensor = transform(face_img).unsqueeze(0).to(next(extractor.parameters()).device)\n        embedding = extractor.base(face_tensor).cpu().numpy()[0]\n        \n        embeddings_dict.setdefault(person, []).append(embedding)\n\n# Save embeddings\nwith open(\"embeddings.pkl\", \"wb\") as f:\n    pickle.dump({\n        \"embeddings_dict\": embeddings_dict,\n        \"label_mapping\": label_to_idx,\n        \"threshold\": 0.5\n    }, f)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluate eval_set.csv\n1. For each test image detect & crop the face\n2. Extract the embedding for this face\n3. Compare it with the embeddings of known people:\n    - Use **cosine similarity** to measure how similar two faces (vectors) are\n    - If the similarity is high enough (above threshold 0.5), assign the face to that person.\n    - Otherwise, \"doesn't_exist\"","metadata":{}},{"cell_type":"code","source":"eval_set = pd.read_csv(EVAL_SET_CSV)\n\nfor idx, row in tqdm(eval_set.iterrows(), total=len(eval_set), desc=\"Evaluating\"):\n    img_path = f\"{TEST_PATH}{row['image_path']}\"\n    \n    # Detect face\n    image = cv2.imread(img_path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    results = detector(image)\n    \n    if len(results[0].boxes) == 0:\n        eval_set.at[idx, \"gt\"] = \"doesn't_exist\"\n        continue\n        \n    # Crop face\n    x1, y1, x2, y2 = map(int, results[0].boxes.xyxy[0].tolist())\n    face_img = image[y1:y2, x1:x2]\n    \n    face_tensor = transform(face_img).unsqueeze(0).to(next(extractor.parameters()).device)\n    with torch.no_grad():\n        query_embedding = extractor.base(face_tensor).cpu().numpy()[0]\n    \n    # Find closest match\n    best_match = None\n    best_sim = -1\n\n    # Compare from dataset using cosine similarity\n    for person, embeddings in embeddings_dict.items():\n        mean_emb = np.mean(embeddings, axis=0)\n        sim = np.dot(query_embedding, mean_emb) / (np.linalg.norm(query_embedding) * np.linalg.norm(mean_emb))\n        if sim > best_sim:\n            best_sim = sim\n            best_match = person\n            \n    eval_set.at[idx, \"gt\"] = best_match if best_sim >= 0.5 else \"doesn't_exist\"\n\neval_set.to_csv(\"eval_set.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Prepare submission.csv file","metadata":{}},{"cell_type":"code","source":"submission = pd.read_csv(f\"/kaggle/input/surveillance-for-retail-stores/submission_file.csv\")\nfilled_eval_set = pd.read_csv(\"eval_set.csv\")\n\nfor index, row in submission.iterrows():\n    if row[\"ID\"] < 429:\n        continue\n    image_dict_string = row[\"objects\"]\n    image_dict = ast.literal_eval(image_dict_string)\n    image_string_name = image_dict[\"image\"]\n    filename = os.path.basename(image_string_name)\n    \n    for index2, row2 in filled_eval_set.iterrows():\n        if row2[\"image_path\"] == filename:\n            ground_truth = row2[\"gt\"]\n            image_dict[\"gt\"] = ground_truth\n            submission.at[index, \"objects\"] = image_dict\n            break\n\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}